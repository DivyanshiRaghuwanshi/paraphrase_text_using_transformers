{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39163897-68df-425c-95f0-98797076efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-splitter in c:\\users\\divya\\anaconda3\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: regex>=2017.12.12 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from sentence-splitter) (2024.9.11)\n",
      "Requirement already satisfied: SentencePiece in c:\\users\\divya\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-splitter\n",
    "! pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9973624-9524-4ddb-998c-23d232188546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\divya\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addc68ce-968c-4566-b1c8-1c727c7ca6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\divya\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\divya\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\divya\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb1c56f-1219-4443-9a13-e1d55cb3db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb177533-bd3d-4cbd-9208-ea82e58407d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fec147a-908f-4406-86fa-a4c5747d90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained('tuner007/pegasus_paraphrase')\n",
    "tokenizer = PegasusTokenizer.from_pretrained('tuner007/pegasus_paraphrase')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7cae3-6005-43ea-b69d-faf584bf38f1",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6e4728-7d59-416c-9cf4-f54dd76e39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "text = \"The ultimate test of your knowledge is your capacity to convey it to another\"\n",
    "\n",
    "batch = tokenizer([text], padding = True, truncation = True, max_length = 60, return_tensors = 'pt')\n",
    "\n",
    "output = model.generate(**batch, max_length = 60, num_beams = 5, num_return_sequences = 5, temperature = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcfe57f-7346-4976-8df6-0498a6023cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The test of your knowledge is your ability to convey it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test of it.',\n",
       " 'The ability to convey your knowledge is the ultimate test of your knowledge.',\n",
       " 'The test of your knowledge is your ability to communicate it.',\n",
       " 'Your capacity to convey your knowledge is the ultimate test.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = tokenizer.batch_decode(output, skip_special_tokens = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7f3f58-6d04-4079-8149-0feae0c6ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 60, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Downloads\\\\tokenizer_config.json',\n",
       " 'Downloads\\\\special_tokens_map.json',\n",
       " 'Downloads\\\\spiece.model',\n",
       " 'Downloads\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save trained model and tokenizer\n",
    "model.save_pretrained(\"Downloads\")\n",
    "tokenizer.save_pretrained(\"Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1cfe19-437f-4326-a82d-2cdf2c070a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model and tokenizer\n",
    "model = PegasusForConditionalGeneration.from_pretrained('Downloads')\n",
    "tokenizer = PegasusTokenizer.from_pretrained('Downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d888cd3-7f21-4644-ac32-f1e76fdce2f6",
   "metadata": {},
   "source": [
    "# Predictive System (Generates ParaPhrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd9029fc-edce-4cdd-b45c-9307720f7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text, num_return_sequences =5, num_beams = 5):\n",
    "    batch = tokenizer([input_text], padding = True, truncation = True, max_length = 60, return_tensors = 'pt')\n",
    "    Translated = model.generate(**batch, max_length = 60, num_beams = num_beams, num_return_sequences = num_return_sequences, temperature = 1.5)\n",
    "    target_text = tokenizer.batch_decode(Translated, skip_special_tokens = True)\n",
    "\n",
    "    return target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c315bdf-0332-4933-a43a-e174a1dcd653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Artificial intelligence is growing in machine learning.',\n",
       " 'Machine learning is growing.',\n",
       " 'Machine learning is gaining in popularity.',\n",
       " 'Machine learning is getting better.',\n",
       " 'Machine learning is a growing field.',\n",
       " 'Machine learning is becoming more and more popular.',\n",
       " 'Machine learning is gaining popularity.',\n",
       " 'Machine learning is growing in popularity.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_beams = 10\n",
    "num_return_sequences = 8\n",
    "context = \"Machine learning is a growing flied in AI\"\n",
    "get_response(context, num_return_sequences, num_beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4178fb4-5ad8-4411-8294-ae487f286124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " PEGASUS Paraphraser \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Enter a sentence to generate paraphrase:\n",
      ">  Lion is a powerful predator and is known as the \"King of the Jungle\" due to its strength and dominance.\n",
      "\n",
      "ðŸ”¹ How many paraphrases do you want? (e.g., 3 to 10):\n",
      ">  10\n",
      "ðŸ”¹ Enter beam search width (Should be equal or greater to the number of paraphrases you want to generate):\n",
      ">  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generating Paraphrases...\n",
      "\n",
      "1. Lion is known as the \"King of the Jungle\" due to its strength and dominance.\n",
      "2. The lion is known as the \"King of the Jungle\" due to its strength and dominance.\n",
      "3. The Lion is known as the \"King of the Jungle\" due to its strength and dominance.\n",
      "4. Lion is known as the \"King of the Jungle\" because of its strength and dominance.\n",
      "5. The lion is known as the \"King of the Jungle\" because of its strength and dominance.\n",
      "6. Lion is the \"King of the Jungle\" due to its strength and dominance.\n",
      "7. The Lion is known as the \"King of the Jungle\" because of its strength and dominance.\n",
      "8. The lion is known as the \"King of the Jungle\" due to its strength.\n",
      "9. Lion is known as the \"King of the Jungle\" due to its strength.\n",
      "10. Lion is the \"King of the Jungle\" because of its strength and dominance.\n"
     ]
    }
   ],
   "source": [
    "# Taking inputs from user:\n",
    "print(\"\\n PEGASUS Paraphraser \\n\")\n",
    "\n",
    "user_input = input(\"ðŸ”¹ Enter a sentence to generate paraphrase:\\n> \")\n",
    "\n",
    "num_outputs = int(input(\"\\nðŸ”¹ How many paraphrases do you want? (e.g., 3 to 10):\\n> \"))\n",
    "beam_width = int(input(\"ðŸ”¹ Enter beam search width (Should be equal or greater to the number of paraphrases you want to generate):\\n> \"))\n",
    "\n",
    "# Generating and showing results here:\n",
    "print(\"\\n Generating Paraphrases...\\n\")\n",
    "paraphrased_sentences = get_response(user_input, num_outputs, beam_width)\n",
    "\n",
    "for i, sent in enumerate(paraphrased_sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4124d-d615-49cf-b7ed-6a2b51e27e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch CUDA Env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
